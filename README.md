# ProdSegmenter

**Real-time semantic segmentation of products on grocery store shelves using Azure-native cloud infrastructure.**

[![Build Status](https://github.com/your-org/prodsegmenter/workflows/CI%2FCD%20Pipeline/badge.svg)](https://github.com/your-org/prodsegmenter/actions)
[![Azure ML](https://img.shields.io/badge/Azure-ML%20Ready-blue)](https://azure.microsoft.com/en-us/services/machine-learning/)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Project Overview

ProdSegmenter is a full-stack computer vision system that performs **real-time semantic segmentation** of products on grocery store shelves from video input (30â€“60 FPS). The system identifies product regions, overlays segmented output on video streams, and provides a web-based interface for visualization and interaction.

### Key Features

- **ğŸš€ Real-time Processing**: 30-60 FPS video segmentation
- **â˜ï¸ Azure-Native**: Fully hosted on Microsoft Azure cloud platform  
- **ğŸ“ˆ Scalable Architecture**: Container-based deployment with auto-scaling
- **ğŸ¨ Web Interface**: Interactive Streamlit frontend for video upload and visualization
- **ğŸ”„ MLOps Pipeline**: Automated model training, evaluation, and deployment
- **ğŸ—ï¸ Production Ready**: Comprehensive testing, monitoring, and CI/CD

---

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "User Interface"
        A[ğŸ‘¤ User] --> B[ğŸ¥ Video Upload/Stream]
        B --> C[ğŸ“± Streamlit Frontend]
    end
    
    subgraph "Azure Cloud Infrastructure"
        C --> D[âš¡ Azure Function API]
        D --> E[ğŸ§  ML Model]
        E --> F[ğŸ“Š Segmentation Results]
        F --> C
        
        G[ğŸ’¾ Azure Blob Storage] --> E
        H[ğŸ‹ï¸ Azure ML] --> G
        I[ğŸ“ˆ Azure Monitor] --> D
        J[ğŸ”§ Azure Container Registry] --> D
    end
    
    subgraph "Development Pipeline"
        K[ğŸ‘¨â€ğŸ’» Developer] --> L[ğŸ™ GitHub]
        L --> M[ğŸ”„ GitHub Actions]
        M --> N[ğŸ§ª Testing]
        N --> O[ğŸ—ï¸ Build Images]
        O --> J
        M --> P[ğŸ“š Model Training]
        P --> H
    end
    
    style A fill:#e1f5fe
    style C fill:#f3e5f5
    style D fill:#e8f5e8
    style E fill:#fff3e0
    style G fill:#fce4ec
    style H fill:#e0f2f1
```

### Data Flow Architecture

```mermaid
sequenceDiagram
    participant U as User
    participant S as Streamlit Frontend
    participant F as Azure Function
    participant M as ML Model
    participant B as Blob Storage
    participant Mon as Azure Monitor
    
    U->>S: Upload Video (30-60 FPS)
    S->>F: POST /api/segment_video
    F->>B: Load Model Weights
    B-->>F: Model Artifacts
    F->>M: Initialize Model
    
    loop For Each Frame
        S->>F: POST /api/segment_frame
        F->>M: Process Frame
        M-->>F: Segmentation Mask
        F->>Mon: Log Metrics (Latency, FPS)
        F-->>S: Return Segmented Frame
        S->>U: Display Overlay
    end
    
    S->>U: Download Processed Video
```

---

## âš™ï¸ Technology Stack

### Infrastructure & Compute
| Component | Technology | Purpose | SLA/Performance |
|-----------|------------|---------|-----------------|
| **Training** | Azure Machine Learning | Model training & experiments | 99.9% uptime |
| **Inference** | Azure Functions | Real-time API endpoints | <33ms latency |
| **Storage** | Azure Blob Storage | Data & model artifacts | 99.999% durability |
| **Frontend** | Azure App Service | Streamlit web interface | Auto-scaling |
| **Registry** | Azure Container Registry | Docker image management | Global replication |
| **Monitoring** | Azure Application Insights | Performance & error tracking | Real-time alerts |

### Machine Learning Stack
| Component | Technology | Use Case | Performance Target |
|-----------|------------|----------|-------------------|
| **Bootstrapping** | Segment Anything Model (SAM) | Initial mask generation | High quality masks |
| **Architecture** | DeepLabV3+/UNet/YOLOv8-Seg | Real-time segmentation | 30-60 FPS |
| **Framework** | PyTorch 2.0+ | Model development | GPU acceleration |
| **Optimization** | ONNX/TensorRT | Inference acceleration | <17ms per frame |
| **Tracking** | MLflow + Azure ML | Experiment management | Version control |

### Development & Deployment
| Tool | Purpose | Integration |
|------|---------|-------------|
| **GitHub Actions** | CI/CD pipeline | Automated testing & deployment |
| **Conda** | Environment management | Reproducible dependencies |
| **Pytest** | Testing framework | 80%+ code coverage |
| **Black/Flake8** | Code quality | Automated formatting |
| **Docker** | Containerization | Multi-stage builds |

---

## ğŸ“Š Performance Requirements

### Real-time Processing Targets

| Model Architecture | Input Size | Target FPS | Latency (ms) | mIoU | GPU Memory |
|-------------------|------------|------------|---------------|------|------------|
| **YOLOv8n-seg** | 640Ã—640 | 60+ | <17 | 0.75+ | 2GB |
| **YOLOv8s-seg** | 640Ã—640 | 45+ | <22 | 0.78+ | 3GB |
| **UNet-ResNet34** | 512Ã—512 | 35+ | <28 | 0.80+ | 4GB |
| **DeepLabV3+-ResNet50** | 512Ã—512 | 30+ | <33 | 0.82+ | 6GB |

### System Scalability

```mermaid
graph LR
    subgraph "Load Balancing"
        A[ğŸ‘¥ 100+ Users] --> B[âš–ï¸ Load Balancer]
    end
    
    subgraph "Auto-scaling"
        B --> C[ğŸ”§ Function App 1]
        B --> D[ğŸ”§ Function App 2]
        B --> E[ğŸ”§ Function App N]
    end
    
    subgraph "Performance Monitoring"
        C --> F[ğŸ“Š App Insights]
        D --> F
        E --> F
        F --> G[ğŸš¨ Alerts]
    end
    
    style A fill:#e3f2fd
    style B fill:#f1f8e9
    style F fill:#fff3e0
```

---

## ğŸ“ Project Structure

```
prodsegmenter/
â”œâ”€â”€ ğŸ“„ README.md                    # Project overview and setup
â”œâ”€â”€ ğŸ“„ environment.yml              # Conda environment
â”œâ”€â”€ ğŸ“„ prodsegmenter_config.yaml   # Project configuration
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“‚ data/                        # Data storage and management
â”‚   â”œâ”€â”€ raw/                        # Raw video files
â”‚   â”œâ”€â”€ processed/                  # Processed frames and masks
â”‚   â””â”€â”€ README.md                   # Data documentation
â”‚
â”œâ”€â”€ ğŸ“‚ notebooks/                   # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_download_dataset.ipynb   # Data acquisition
â”‚   â”œâ”€â”€ 02_sam_bootstrap.ipynb      # SAM mask generation
â”‚   â”œâ”€â”€ 03_preprocessing.ipynb      # Data preprocessing
â”‚   â””â”€â”€ README.md                   # Notebook documentation
â”‚
â”œâ”€â”€ ğŸ“‚ models/                      # Model storage
â”‚   â”œâ”€â”€ sam/                        # SAM model weights
â”‚   â”œâ”€â”€ custom_segmentation/        # Custom trained models
â”‚   â””â”€â”€ README.md                   # Model documentation
â”‚
â”œâ”€â”€ ğŸ“‚ training/                    # Training pipeline
â”‚   â”œâ”€â”€ train.py                    # Model training script
â”‚   â”œâ”€â”€ evaluate.py                 # Model evaluation
â”‚   â”œâ”€â”€ dataset.py                  # Dataset classes
â”‚   â””â”€â”€ README.md                   # Training documentation
â”‚
â”œâ”€â”€ ğŸ“‚ deployment/                  # Deployment assets
â”‚   â”œâ”€â”€ azure_function/             # Azure Function API
â”‚   â”œâ”€â”€ streamlit_frontend/         # Web interface
â”‚   â””â”€â”€ README.md                   # Deployment documentation
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                       # Test suite
â”‚   â”œâ”€â”€ test_train.py               # Training tests
â”‚   â”œâ”€â”€ test_inference.py           # Inference tests
â”‚   â””â”€â”€ README.md                   # Testing documentation
â”‚
â””â”€â”€ ğŸ“‚ .github/                     # CI/CD workflows
    â””â”€â”€ workflows/
        â””â”€â”€ ci.yml                  # GitHub Actions CI
```

---

## ğŸš€ Quick Start Guide

### Prerequisites

- **Azure Subscription** with appropriate permissions
- **Git** installed locally
- **Conda/Miniconda** installed
- **Azure CLI** installed and configured

### 1. ğŸ”§ Azure Environment Setup

```bash
# Login to Azure
az login

# Set your subscription
az account set --subscription "your-subscription-id"

# Create resource group
az group create --name rg-prodsegmenter --location eastus

# Create Azure ML workspace
az ml workspace create --name ml-prodsegmenter --resource-group rg-prodsegmenter
```

### 2. ğŸ“¥ Project Setup

```bash
# Clone the repository
git clone https://github.com/your-org/prodsegmenter.git
cd prodsegmenter

# Create and activate conda environment
conda env create -f environment.yml
conda activate prodsegmenter

# Initialize git (if not cloned)
git init
git add .
git commit -m "Initial project setup"
```

### 3. âš™ï¸ Configuration

```bash
# Copy configuration template
cp prodsegmenter_config.yaml.template prodsegmenter_config.yaml

# Edit configuration with your Azure details
nano prodsegmenter_config.yaml
```

Set the following environment variables:
```bash
export AZURE_SUBSCRIPTION_ID="your-subscription-id"
export AZURE_RESOURCE_GROUP="rg-prodsegmenter"
export AZURE_ML_WORKSPACE="ml-prodsegmenter"
export AZURE_STORAGE_ACCOUNT="stprodsegmenter"
```

### 4. ğŸ§ª Verification

```bash
# Test environment setup
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

# Test Azure connectivity
az ml workspace show --name ml-prodsegmenter --resource-group rg-prodsegmenter
```

---

## ğŸ“ˆ Development Workflow

### Phase-by-Phase Implementation

```mermaid
gantt
    title ProdSegmenter Development Timeline
    dateFormat  YYYY-MM-DD
    section Phase 1: Foundation
    Project Setup           :done, setup, 2024-01-01, 2024-01-03
    Data Acquisition        :active, data, 2024-01-04, 2024-01-06
    SAM Bootstrapping       :sam, 2024-01-07, 2024-01-09
    
    section Phase 2: Model Development
    Data Preprocessing      :prep, 2024-01-10, 2024-01-12
    Model Training          :train, 2024-01-13, 2024-01-17
    Model Evaluation        :eval, 2024-01-18, 2024-01-20
    
    section Phase 3: Deployment
    API Development         :api, 2024-01-21, 2024-01-24
    Frontend Development    :frontend, 2024-01-25, 2024-01-27
    Testing & CI/CD         :testing, 2024-01-28, 2024-01-30
    
    section Phase 4: Production
    Monitoring Setup        :monitor, 2024-01-31, 2024-02-02
    Performance Optimization :optimize, 2024-02-03, 2024-02-05
    Documentation           :docs, 2024-02-06, 2024-02-07
```

### ğŸ”„ MLOps Pipeline

```mermaid
flowchart LR
    subgraph "Data Pipeline"
        A[ğŸ“¹ Raw Videos] --> B[ğŸ”„ Preprocessing]
        B --> C[ğŸ·ï¸ SAM Labeling]
        C --> D[ğŸ“Š Training Data]
    end
    
    subgraph "Model Pipeline"
        D --> E[ğŸ‹ï¸ Training]
        E --> F[ğŸ“ Evaluation]
        F --> G{âœ… Quality Gate}
        G -->|Pass| H[ğŸ“¦ Model Registry]
        G -->|Fail| E
    end
    
    subgraph "Deployment Pipeline"
        H --> I[ğŸš€ Staging Deploy]
        I --> J[ğŸ§ª Integration Tests]
        J --> K{âœ… Test Gate}
        K -->|Pass| L[ğŸŒ Production Deploy]
        K -->|Fail| I
    end
    
    subgraph "Monitoring"
        L --> M[ğŸ“Š Performance Metrics]
        M --> N[ğŸš¨ Alerts]
        N --> O{ğŸ“‰ Drift Detected?}
        O -->|Yes| E
        O -->|No| M
    end
    
    style A fill:#e3f2fd
    style H fill:#e8f5e8
    style L fill:#fff3e0
    style N fill:#ffebee
```

---

## ğŸ§ª Development Commands

### Data Processing
```bash
# Extract and preprocess data
jupyter lab notebooks/01_download_dataset.ipynb

# Generate SAM masks
jupyter lab notebooks/02_sam_bootstrap.ipynb

# Prepare training data
jupyter lab notebooks/03_preprocessing.ipynb
```

### Model Training
```bash
# Local training (quick test)
cd training/
python train.py --config ../prodsegmenter_config.yaml --epochs 5

# Azure ML training (full)
python train.py --config ../prodsegmenter_config.yaml --azure
```

### Testing
```bash
# Run all tests
pytest tests/ -v --cov=training --cov=deployment

# Run specific test categories
pytest tests/test_train.py -v
pytest tests/test_inference.py -v
```

### Deployment
```bash
# Local development
cd deployment/streamlit_frontend/
streamlit run app.py

# Azure deployment
cd deployment/azure_function/
func azure functionapp publish func-prodsegmenter-inference
```

---

## ğŸ“Š Monitoring & Analytics

### Key Metrics Dashboard

| Metric Category | Key Indicators | Target Values | Alerting |
|----------------|----------------|---------------|----------|
| **Performance** | Response Time, FPS, Throughput | <33ms, >30 FPS, >100 req/s | >50ms response |
| **Accuracy** | mIoU, Pixel Accuracy, F1 Score | >0.80, >0.90, >0.85 | <0.75 mIoU |
| **Reliability** | Uptime, Error Rate, Success Rate | >99.9%, <1%, >99% | >5% error rate |
| **Resource** | CPU, Memory, GPU Utilization | <80%, <85%, <90% | >95% utilization |

### Architecture Decision Records (ADRs)

```mermaid
graph TD
    A[ğŸ¤” Architecture Decision] --> B{ğŸ¯ Real-time Requirement?}
    B -->|Yes| C[âš¡ Azure Functions]
    B -->|No| D[ğŸ—ï¸ Azure Container Apps]
    
    C --> E{ğŸ“Š Model Size?}
    E -->|Small| F[ğŸš€ YOLOv8n-seg]
    E -->|Large| G[ğŸ§  DeepLabV3+]
    
    D --> H{ğŸ’° Cost Priority?}
    H -->|High| I[ğŸ“¦ Consumption Plan]
    H -->|Low| J[ğŸƒ Premium Plan]
    
    style A fill:#e1f5fe
    style C fill:#e8f5e8
    style F fill:#fff3e0
    style G fill:#f3e5f5
```

---

## ğŸ¤ Contributing

### Development Process

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-awesome-improvement`
3. **Make** changes and add tests
4. **Run** the test suite: `pytest tests/ --cov=80`
5. **Format** code: `black . && flake8 .`
6. **Submit** a pull request with detailed description

### Code Quality Standards

- **Test Coverage**: Minimum 80% overall, 95% for core modules
- **Documentation**: All public functions must have docstrings
- **Performance**: Real-time functions must meet latency requirements
- **Security**: All inputs must be validated and sanitized

---

## ğŸ“š Additional Resources

### Documentation Links
- [ğŸ“– Azure ML Documentation](https://docs.microsoft.com/en-us/azure/machine-learning/)
- [ğŸ”§ Azure Functions Guide](https://docs.microsoft.com/en-us/azure/azure-functions/)
- [ğŸ¨ Streamlit Documentation](https://docs.streamlit.io/)
- [ğŸ¤– SAM Model Documentation](https://github.com/facebookresearch/segment-anything)

### Learning Resources
- [ğŸ“ Computer Vision Course](https://cs231n.stanford.edu/)
- [â˜ï¸ Azure ML Learning Path](https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-machine-learning-service/)
- [ğŸ PyTorch Tutorials](https://pytorch.org/tutorials/)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support & Contact

- **ğŸ“§ Email**: support@prodsegmenter.com
- **ğŸ’¬ Discord**: [ProdSegmenter Community](https://discord.gg/prodsegmenter)
- **ğŸ“š Wiki**: [Documentation](https://github.com/your-org/prodsegmenter/wiki)
- **ğŸ› Issues**: [GitHub Issues](https://github.com/your-org/prodsegmenter/issues)

---

## ğŸ¯ Current Status

**âœ… Prompt 0 & 1 Complete**: Architecture defined, project bootstrapped, ready for data acquisition

**ğŸš€ Next Steps**: 
- Proceed to Prompt 2: Download and organize grocery shelf datasets
- Set up SKU110K dataset processing
- Begin SAM bootstrap pipeline

---

*Built with â¤ï¸ by the ProdSegmenter team using Azure cloud infrastructure* # ProdSegmenter
